{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ML_Model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eq4hlIJp9xhw","executionInfo":{"status":"ok","timestamp":1650922949538,"user_tz":240,"elapsed":1141,"user":{"displayName":"Kamilla Muminova","userId":"04376261050601407819"}},"outputId":"cf2cd02b-a5d7-41f0-82d0-66a5358b3a8a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive \n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["#import statements for numpy\n","import numpy as np\n","\n","#import statements for sklearn\n","from sklearn import datasets, linear_model\n","from sklearn.model_selection import cross_validate, train_test_split\n","from sklearn.metrics import make_scorer, accuracy_score, roc_auc_score, confusion_matrix, auc\n","from sklearn.svm import LinearSVC\n","from sklearn.linear_model import LogisticRegression\n","\n","#import seabord to create an accuracy score and plot\n","import seaborn as sn\n","\n","#import pyplot\n","import matplotlib.pyplot as plt\n","\n","#import for pandas\n","import pandas as pd\n"],"metadata":{"id":"gcg8Y5VzElzN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## create a matrix using Yifan's exported cvs file regarding the LIWC features. Note that you need to locate the features that are LIWC categories only \n","## and disregard the remaining ones. \n","\n","## For each row, find the corresponding feature vector and the class label (1-Yes, 0-No). Then train a logistic regression model:\n","## So, here the columns that are Category labels should be apart of the feature vector and the output is the realOrFake column\n","## Lets do this by reading in the CSV file as a Pandas Dataframe, then converting it to a matrix. \n","\n","#read in the csv file into a dataframe\n","liwc_df = pd.read_csv ('/content/gdrive/MyDrive/CSE-CSB_Capstone/Subteam-1/LIWC-outputOnContentsAndNY_big_cat.csv')\n","\n","#check that this worked\n","#liwc_df.head(n=2)\n","\n","\n","# create a logistic regression model based on categories, choosing realOrFake to be what we are predicting. \n","# referenced from how it was done on https://datatofish.com/logistic-regression-python/\n","\n","# feature vector of categories being used for prediction\n","x = liwc_df[['Linguistic', 'Drives','Cognition','Affect','Social','Culture','Lifestyle', 'Physical', 'Perception', 'Conversation']]\n","\n","# category we are predicting\n","y = liwc_df['realOrFake']\n","\n","# split the data into training data and testing data, with 1/4 being reserved for testings \n","# Make sure that the positive class is included (keep ratio of the original yes and no)\n","# split negative and positives seperately and then by ratio combine them \n","x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=0)\n","\n","\n","# create a new Logistic Regression tool\n","logistic_regression = LogisticRegression(solver=\"sag\", max_iter=5000)\n","\n","\n","# fit the training data to the model\n","logistic_regression.fit(x_train,y_train)\n","\n","#predict the realOrFake category based on the logistic regression applied to the testing portion of the data\n","y_pred = logistic_regression.predict(x_test)\n","y_pred_prob = logistic_regression.predict_proba(x_test)\n","\n","#print the accuracy of the model by comparing the testing data against the real predictions\n","print('Accuracy: ', accuracy_score(y_test, y_pred))\n","print('AUC:', roc_auc_score(y_test, y_pred_prob[:,-1]))\n","\n","#predict probability function (how likely the review is fake), allows us to evaluate the AUC curve\n","\n","#debug with Prof at another time\n","#plug in our model into the cross validation from sklearn\n","cv_results = cross_validate(logistic_regression, x, y, cv=None)\n","print('Cross validation: ', cv_results)\n","\n","\n","\n","\n","\n","# Get the confusion matrix\n","#confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\n","\n","#plt.show()\n","#sn.heatmap(confusion_matrix, annot=True)\n","#convert the whole dataframe into a numpy array, not sure if this is necessary\n","#liwc_matrix = liwc_df.to_numpy()\n","#print(liwc_matrix[0])\n","\n","\n","\n","## https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n","## and plug in the logistic regression model in the cross-validation:\n","## https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html\n","## The expected output is a report of the performance of the model.\n","\n","#https://stackoverflow.com/questions/36681449/scikit-learn-return-value-of-logisticregression-predict-proba\n","\n","\n"],"metadata":{"id":"RRS0MA4q_7gQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650923240379,"user_tz":240,"elapsed":6960,"user":{"displayName":"Kamilla Muminova","userId":"04376261050601407819"}},"outputId":"8cf10a4b-e686-460e-b622-fdd3dc200708"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy:  0.8633879781420765\n","AUC: 0.5742731341071078\n","Cross validation:  {'fit_time': array([0.89609647, 0.95233035, 1.28446627, 1.02932572, 1.08730054]), 'score_time': array([0.00470877, 0.00279427, 0.00471759, 0.00291538, 0.0028801 ]), 'test_score': array([0.86592656, 0.86678053, 0.86678053, 0.86678053, 0.86666667])}\n"]}]},{"cell_type":"code","source":["## create a matrix using Yifan's exported cvs file regarding the LIWC features. Note that you need to locate the features that are LIWC categories only \n","## and disregard the remaining ones. \n","\n","## For each row, find the corresponding feature vector and the class label (1-Yes, 0-No). Then train a logistic regression model:\n","## So, here the columns that are Category labels should be apart of the feature vector and the output is the realOrFake column\n","## Lets do this by reading in the CSV file as a Pandas Dataframe, then converting it to a matrix. \n","\n","#read in the csv file into a dataframe\n","liwc_df = pd.read_csv ('/content/gdrive/MyDrive/CSE-CSB_Capstone/Subteam-1/LIWC-outputOnContentsAndNY_small_cat.csv')\n","LIEC_result=liwc_df\n","fake_df=LIWC_result[LIWC_result.realOrFake==0].drop([\"realOrFake\",\"Unnamed: 0\",\"content\",\"unknown1\",\"unknown2\",\"unknown3\",\"Segment\",\"dates\",\"reviewID\",\"reviewerID\",\"productID\"],axis=\"columns\")\n","real_df=LIWC_result[LIWC_result.realOrFake==1].drop([\"realOrFake\",\"Unnamed: 0\",\"content\",\"unknown1\",\"unknown2\",\"unknown3\",\"Segment\",\"dates\",\"reviewID\",\"reviewerID\",\"productID\"],axis=\"columns\")\n","\n","#check that this worked\n","#liwc_df.head(n=2)\n","\n","\n","# create a logistic regression model based on categories, choosing realOrFake to be what we are predicting. \n","# referenced from how it was done on https://datatofish.com/logistic-regression-python/\n","\n","# feature vector of categories being used for prediction\n","x = liwc_df[['function', 'pronoun','ppron','i','we','you','shehe', 'they', 'ipron', 'det', \n","             'article','number','prep','auxverb','adverb','conj', 'negate', 'verb', 'adj',\n","             'quantity','affiliation','achieve','power','allnone', 'cogproc', 'insight', 'cause',\n","             'discrep','certitude','differ','memory','tone_pos', 'tone_neg', 'emotion', 'emo_pos',\n","             'emo_neg','emo_anx','emo_anger','emo_sad','swear', 'socbehav', 'prosocial', 'polite',\n","             'conflict','moral','comm','socrefs','family', 'friend', 'female', 'male',\n","             'politic','ethnicity','tech','leisure','home', 'work', 'money', 'relig',\n","             'health','illness','wellness','mental','substances', 'sexual', 'food', 'death',\n","             'need','want','acquire','lack','fulfill', 'fatigue', 'reward', 'risk','curiosity', \n","             'allure','attention','motion','space', 'visual', 'auditory', 'feeling','time',\n","             'focuspast','focuspresent','focusfuture','netspeak', 'assent', 'nonflu', 'filler']]\n","\n","# category we are predicting\n","y = liwc_df['realOrFake']\n","\n","# split the data into training data and testing data, with 1/4 being reserved for testings \n","# Make sure that the positive class is included (keep ratio of the original yes and no)\n","# split negative and positives seperately and then by ratio combine them \n","x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=0)\n","\n","\n","# create a new Logistic Regression tool\n","logistic_regression = LogisticRegression(solver=\"sag\", max_iter=2000)\n","\n","\n","# fit the training data to the model\n","logistic_regression.fit(x_train,y_train)\n","\n","#predict the realOrFake category based on the logistic regression applied to the testing portion of the data\n","y_pred = logistic_regression.predict(x_test)\n","y_pred_prob = logistic_regression.predict_proba(x_test)\n","\n","#print the accuracy of the model by comparing the testing data against the real predictions\n","print('Accuracy: ', accuracy_score(y_test, y_pred))\n","print('AUC:', roc_auc_score(y_test, y_pred_prob[:,-1]))\n","\n","\n","#predict probability function (how likely the review is fake), allows us to evaluate the AUC curve\n","\n","#debug with Prof at another time\n","#plug in our model into the cross validation from sklearn\n","cv_results = cross_validate(logistic_regression, x, y, cv=None)\n","\n","\n","print('Cross validation: ', cv_results)\n","\n","# Get the confusion matrix\n","#confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\n","\n","#plt.show()\n","#sn.heatmap(confusion_matrix, annot=True)\n","#convert the whole dataframe into a numpy array, not sure if this is necessary\n","#liwc_matrix = liwc_df.to_numpy()\n","#print(liwc_matrix[0])\n","\n","\n","\n","## https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n","## and plug in the logistic regression model in the cross-validation:\n","## https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html\n","## The expected output is a report of the performance of the model.\n","\n","#https://stackoverflow.com/questions/36681449/scikit-learn-return-value-of-logisticregression-predict-proba\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":270},"id":"8aGBLzZt0EFY","executionInfo":{"status":"error","timestamp":1650923192179,"user_tz":240,"elapsed":1169,"user":{"displayName":"Kamilla Muminova","userId":"04376261050601407819"}},"outputId":"bfcd3da4-1ecc-48d5-edb7-2a2baff0e5c2"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-445c8386b469>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mliwc_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/MyDrive/CSE-CSB_Capstone/Subteam-1/LIWC-outputOnContentsAndNY_small_cat.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mLIEC_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mliwc_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mfake_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLIWC_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLIWC_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealOrFake\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"realOrFake\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Unnamed: 0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"unknown1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"unknown2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"unknown3\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Segment\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"dates\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"reviewID\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"reviewerID\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"productID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mreal_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLIWC_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLIWC_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealOrFake\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"realOrFake\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Unnamed: 0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"unknown1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"unknown2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"unknown3\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Segment\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"dates\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"reviewID\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"reviewerID\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"productID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'LIWC_result' is not defined"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"VirUWFTyooKy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Implement more features (either high level categories or low level categories)\n","#Hack into the dictionary jar file to get the dictionary out\n","\n","\n","#further analyze the prediction of the logistic regression (see why it is wrong)\n","# identify which row the model does wrong\n","# print out the original feature \n","\n","\n","\n","\n","#try Naive bayes or Decision Trees\n","\n","#seperate all 1s and 0s into two lists\n","#shuffle and split these two lists into 5 different things\n","#and then "],"metadata":{"id":"XR5etj0Fr1Z-"},"execution_count":null,"outputs":[]}]}